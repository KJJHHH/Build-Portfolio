# Build Stock Portfolio
### Sub-Project: [Regression Analysis](https://github.com/KJJHHH/Build-Portfolio/tree/master/TEJ_portfolio)

ğŸ± Building stock portfolio utilising different machine learning algorithm including linear regressions, SVM, tree methods, and deep learning.
The portfolios are based on automobile, semi conductor, and TFT-LCD industry.
ğŸ± Training the algorithms with rolling prediction for each month.

### ğŸ¦ Data and Preprocessing
ğŸ˜¹ The data for training algorithms are downloaded from TEJ database, including fundimental, chip, and [Betas](https://api.tej.com.tw/columndoc.html?subId=51) data. Besides, we compute the technical analysis values include RSI, SMA, EMA, MACD, bband, KD, beta, willr, and bias with [talib packages](https://github.com/TA-Lib/ta-lib-python?tab=readme-ov-file#indicator-groups) \
ğŸ“ [Tutorial for talib](https://medium.com/ai%E8%82%A1%E4%BB%94/%E7%94%A8-python-%E5%BF%AB%E9%80%9F%E8%A8%88%E7%AE%97-158-%E7%A8%AE%E6%8A%80%E8%A1%93%E6%8C%87%E6%A8%99-26f9579b8f3a)
ğŸ–¼ï¸ Update date for monthly portfolio: The last revenue announce date for all companies. If any announcement date is later than 12th, delete the company for the month
ğŸ—£ï¸ For each prediction in rolling prediction, use five-year training data to predict the adjacent month



### ğŸ¦ Algorithm
ğŸ˜¹ Implement different machine learning algorithm and compare the results. The algorithms include:
- Multiple linear regression
- Elastic Net
- Decision tree
- Random forest
- Xgboost
- SVM
- Neural Network
ğŸ˜¹ For each algorithms, we utilise grid search or random search with training data.
- [Machine learning tuning with sklearn](https://scikit-learn.org/stable/modules/grid_search.html)
- [Deep learning tuning with optuna](https://github.com/optuna/optuna)
```
def params_setting(model):
    if model == "linear":
        params = {
            # "normalize": True        
        }
    elif model == "decision tree":
        params = { # decision tree
            "criterion": ['friedman_mse', 'absolute_error', 'poisson', 'squared_error'],  # squared_e defaulted
            "max_depth": [None, 5, 10], # 
            "min_samples_split": [5, 10],
        }
    elif model == "random forest":
        params = {
            'n_estimators': [20], # 50, 100, 
            'max_depth': [None, 10, 20],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4],
            'max_features': ['auto', 'sqrt', 'log2']
        }
    elif model == "xgboost":
        params = {
            "learning_rate": [0.01, 0.1, 0.001],
            "n_estimators": [5, 10, 20, 30], # original set: [5, 10, 20, 30]
            "max_depth": [None, 3, 10, 5],
            "min_child_weight": [1, 2, 3] 
        }
    elif model == "svm": 
        params = {
            "C": [0.1, 1, 10],
            "kernel": ["rbf"], # è·‘å¤ªä¹…ï¼š, "poly", "linear"
            "gamma": ["scale", "auto", 0.1, 1]
        }
    elif model == "neural network": # tune in random, so only set the n_trials of tune
        params = {
            "batch_size": 25,
            }
    elif model == "elastic net":
        params = {
            "l1_ratio" : np.arange(0., 1., .1),
            "alpha" : [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1],
        }
    elif model == "ensemble":
        params = {}
    return params
```



ğŸ™‰

ğŸ˜¹
ğŸ§‘
ğŸ’—
ğŸ¦
ğŸ‹